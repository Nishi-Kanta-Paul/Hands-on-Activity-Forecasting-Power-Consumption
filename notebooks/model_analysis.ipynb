{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete!\n",
      "Original Data Shape: (52416, 9)\n",
      "\n",
      "First few rows of original data:\n",
      "        Datetime  Temperature  Humidity  WindSpeed  GeneralDiffuseFlows  \\\n",
      "0  1/1/2017 0:00        6.559      73.8      0.083                0.051   \n",
      "1  1/1/2017 0:10        6.414      74.5      0.083                0.070   \n",
      "2  1/1/2017 0:20        6.313      74.5      0.080                0.062   \n",
      "3  1/1/2017 0:30        6.121      75.0      0.083                0.091   \n",
      "4  1/1/2017 0:40        5.921      75.7      0.081                0.048   \n",
      "\n",
      "   DiffuseFlows  PowerConsumption_Zone1  PowerConsumption_Zone2  \\\n",
      "0         0.119             34055.69620             16128.87538   \n",
      "1         0.085             29814.68354             19375.07599   \n",
      "2         0.100             29128.10127             19006.68693   \n",
      "3         0.096             28228.86076             18361.09422   \n",
      "4         0.085             27335.69620             17872.34043   \n",
      "\n",
      "   PowerConsumption_Zone3  \n",
      "0             20240.96386  \n",
      "1             20131.08434  \n",
      "2             19668.43373  \n",
      "3             18899.27711  \n",
      "4             18442.40964  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed\\\\train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m results_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/results\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load preprocessed data\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m val_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(preprocessed_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     30\u001b[0m test_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(preprocessed_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32me:\\USA\\Task\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed\\\\train_data.npy'"
     ]
    }
   ],
   "source": [
    "# First Cell - Imports and Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "print(\"Setup Complete!\")\n",
    "\n",
    "# Second Cell - Load Data\n",
    "# Load original data\n",
    "data = pd.read_csv('../data/powerconsumption.csv')\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"\\nFirst few rows of original data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_path = '../data/processed'\n",
    "results_path = '../data/results'\n",
    "\n",
    "# Load preprocessed data\n",
    "train_data = np.load(os.path.join(preprocessed_path, 'train_data.npy'))\n",
    "val_data = np.load(os.path.join(preprocessed_path, 'val_data.npy'))\n",
    "test_data = np.load(os.path.join(preprocessed_path, 'test_data.npy'))\n",
    "\n",
    "print(\"\\nPreprocessed Data Shapes:\")\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {val_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")\n",
    "\n",
    "# Load model results\n",
    "transformer_results = np.load(os.path.join(results_path, 'transformer_predictions.npy'))\n",
    "patchtst_results = np.load(os.path.join(results_path, 'patchtst_predictions.npy'))\n",
    "actual_values = np.load(os.path.join(results_path, 'actual_values.npy'))\n",
    "\n",
    "print(\"\\nModel Results Shapes:\")\n",
    "print(f\"Transformer predictions: {transformer_results.shape}\")\n",
    "print(f\"PatchTST predictions: {patchtst_results.shape}\")\n",
    "print(f\"Actual values: {actual_values.shape}\")\n",
    "\n",
    "# Third Cell - Calculate Metrics\n",
    "def calculate_metrics(y_true, y_pred, zone_idx):\n",
    "    \"\"\"Calculate comprehensive metrics for a specific zone\"\"\"\n",
    "    mae = mean_absolute_error(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    mse = mean_squared_error(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each zone and model\n",
    "zones = ['Zone 1', 'Zone 2', 'Zone 3']\n",
    "models = ['Transformer', 'PatchTST']\n",
    "results = {}\n",
    "\n",
    "for zone_idx, zone in enumerate(zones):\n",
    "    results[zone] = {\n",
    "        'Transformer': calculate_metrics(actual_values, transformer_results, zone_idx),\n",
    "        'PatchTST': calculate_metrics(actual_values, patchtst_results, zone_idx)\n",
    "    }\n",
    "\n",
    "# Display results as a DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    (zone, model, metric): value\n",
    "    for zone in zones\n",
    "    for model in models\n",
    "    for metric, value in results[zone][model].items()\n",
    "}).unstack(level=[1, 2])\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (52416, 9)\n",
      "\n",
      "First few rows of original data:\n",
      "        Datetime  Temperature  Humidity  WindSpeed  GeneralDiffuseFlows  \\\n",
      "0  1/1/2017 0:00        6.559      73.8      0.083                0.051   \n",
      "1  1/1/2017 0:10        6.414      74.5      0.083                0.070   \n",
      "2  1/1/2017 0:20        6.313      74.5      0.080                0.062   \n",
      "3  1/1/2017 0:30        6.121      75.0      0.083                0.091   \n",
      "4  1/1/2017 0:40        5.921      75.7      0.081                0.048   \n",
      "\n",
      "   DiffuseFlows  PowerConsumption_Zone1  PowerConsumption_Zone2  \\\n",
      "0         0.119             34055.69620             16128.87538   \n",
      "1         0.085             29814.68354             19375.07599   \n",
      "2         0.100             29128.10127             19006.68693   \n",
      "3         0.096             28228.86076             18361.09422   \n",
      "4         0.085             27335.69620             17872.34043   \n",
      "\n",
      "   PowerConsumption_Zone3  \n",
      "0             20240.96386  \n",
      "1             20131.08434  \n",
      "2             19668.43373  \n",
      "3             18899.27711  \n",
      "4             18442.40964  \n",
      "\n",
      "Preprocessed data not found. Please run preprocessing first:\n",
      "python src/data_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "# Second Cell - Load Data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define paths relative to notebook location\n",
    "DATA_DIR = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "\n",
    "# Load original data\n",
    "data = pd.read_csv(os.path.join(DATA_DIR, 'powerconsumption.csv'))\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"\\nFirst few rows of original data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    train_data = np.load(os.path.join(DATA_DIR, 'processed/train_data.npy'))\n",
    "    val_data = np.load(os.path.join(DATA_DIR, 'processed/val_data.npy'))\n",
    "    test_data = np.load(os.path.join(DATA_DIR, 'processed/test_data.npy'))\n",
    "    \n",
    "    print(\"\\nLoaded Preprocessed Data Shapes:\")\n",
    "    print(f\"Train data: {train_data.shape}\")\n",
    "    print(f\"Validation data: {val_data.shape}\")\n",
    "    print(f\"Test data: {test_data.shape}\")\n",
    "    \n",
    "    # Load preprocessing info\n",
    "    with open(os.path.join(DATA_DIR, 'processed/preprocessing_info.json'), 'r') as f:\n",
    "        preprocessing_info = json.load(f)\n",
    "    print(\"\\nPreprocessing Info:\")\n",
    "    print(json.dumps(preprocessing_info, indent=2))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nPreprocessed data not found. Please run preprocessing first:\")\n",
    "    print(\"python src/data_preprocessing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actual_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zone_idx, zone \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(zones):\n\u001b[0;32m     34\u001b[0m     results[zone] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: calculate_metrics(\u001b[43mactual_values\u001b[49m, transformer_results, zone_idx),\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatchTST\u001b[39m\u001b[38;5;124m'\u001b[39m: calculate_metrics(actual_values, patchtst_results, zone_idx)\n\u001b[0;32m     37\u001b[0m     }\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create and display metrics DataFrame\u001b[39;00m\n\u001b[0;32m     40\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     41\u001b[0m     (zone, model, metric): value\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m zone \u001b[38;5;129;01min\u001b[39;00m zones\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m results[zone][model]\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     45\u001b[0m })\u001b[38;5;241m.\u001b[39munstack(level\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actual_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Third Cell - Evaluation Metrics Implementation\n",
    "def calculate_metrics(y_true, y_pred, zone_idx):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for a specific zone\n",
    "    Args:\n",
    "        y_true: Actual values\n",
    "        y_pred: Predicted values\n",
    "        zone_idx: Index of the zone (0, 1, or 2)\n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    mse = mean_squared_error(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true[:, zone_idx], y_pred[:, zone_idx])\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    mape = np.mean(np.abs((y_true[:, zone_idx] - y_pred[:, zone_idx]) / y_true[:, zone_idx])) * 100\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each zone and model\n",
    "zones = ['Zone 1', 'Zone 2', 'Zone 3']\n",
    "models = ['Transformer', 'PatchTST']\n",
    "results = {}\n",
    "\n",
    "for zone_idx, zone in enumerate(zones):\n",
    "    results[zone] = {\n",
    "        'Transformer': calculate_metrics(actual_values, transformer_results, zone_idx),\n",
    "        'PatchTST': calculate_metrics(actual_values, patchtst_results, zone_idx)\n",
    "    }\n",
    "\n",
    "# Create and display metrics DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    (zone, model, metric): value\n",
    "    for zone in zones\n",
    "    for model in models\n",
    "    for metric, value in results[zone][model].items()\n",
    "}).unstack(level=[1, 2])\n",
    "\n",
    "print(\"Detailed Performance Metrics:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth Cell - Visualization of Predictions\n",
    "def plot_predictions(actual, transformer_pred, patchtst_pred, zone_idx, zone_name):\n",
    "    \"\"\"Plot actual vs predicted values for a specific zone\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot with different styles for better visibility\n",
    "    plt.plot(actual[:100, zone_idx], 'b-', label='Actual', linewidth=2, alpha=0.7)\n",
    "    plt.plot(transformer_pred[:100, zone_idx], 'r--', label='Transformer', linewidth=2, alpha=0.7)\n",
    "    plt.plot(patchtst_pred[:100, zone_idx], 'g:', label='PatchTST', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Power Consumption Predictions - {zone_name}', fontsize=14)\n",
    "    plt.xlabel('Time Steps', fontsize=12)\n",
    "    plt.ylabel('Power Consumption', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add error bands\n",
    "    transformer_error = np.abs(actual[:100, zone_idx] - transformer_pred[:100, zone_idx])\n",
    "    patchtst_error = np.abs(actual[:100, zone_idx] - patchtst_pred[:100, zone_idx])\n",
    "    \n",
    "    plt.fill_between(range(100), \n",
    "                     actual[:100, zone_idx] - transformer_error,\n",
    "                     actual[:100, zone_idx] + transformer_error,\n",
    "                     color='red', alpha=0.1)\n",
    "    plt.fill_between(range(100), \n",
    "                     actual[:100, zone_idx] - patchtst_error,\n",
    "                     actual[:100, zone_idx] + patchtst_error,\n",
    "                     color='green', alpha=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot predictions for each zone\n",
    "for idx, zone in enumerate(zones):\n",
    "    plot_predictions(actual_values, transformer_results, patchtst_results, idx, zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth Cell - Performance Metrics Comparison\n",
    "def plot_metrics_comparison():\n",
    "    \"\"\"Plot comparison of different metrics across models and zones\"\"\"\n",
    "    metrics_to_plot = ['RMSE', 'MAE', 'R2', 'MAPE']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        data = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        for zone in zones:\n",
    "            for model in models:\n",
    "                data.append(results[zone][model][metric])\n",
    "                labels.append(f'{zone}\\n{model}')\n",
    "                colors.append('skyblue' if model == 'Transformer' else 'lightgreen')\n",
    "        \n",
    "        axes[idx].bar(labels, data, color=colors)\n",
    "        axes[idx].set_title(f'{metric} Comparison', fontsize=14)\n",
    "        axes[idx].set_ylabel(metric, fontsize=12)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for i, v in enumerate(data):\n",
    "            axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixth Cell - Detailed Analysis and Conclusions\n",
    "print(\"Model Performance Analysis\")\n",
    "print(\"=========================\")\n",
    "\n",
    "print(\"\\n1. Overall Model Performance:\")\n",
    "for zone in zones:\n",
    "    print(f\"\\n{zone}:\")\n",
    "    transformer_metrics = results[zone]['Transformer']\n",
    "    patchtst_metrics = results[zone]['PatchTST']\n",
    "    \n",
    "    # Determine better model\n",
    "    better_model = 'Transformer' if transformer_metrics['RMSE'] < patchtst_metrics['RMSE'] else 'PatchTST'\n",
    "    improvement = abs(transformer_metrics['RMSE'] - patchtst_metrics['RMSE'])\n",
    "    \n",
    "    print(f\"  Best Model: {better_model}\")\n",
    "    print(f\"  RMSE Improvement: {improvement:.4f}\")\n",
    "    print(f\"  Transformer Metrics: RMSE={transformer_metrics['RMSE']:.4f}, R2={transformer_metrics['R2']:.4f}\")\n",
    "    print(f\"  PatchTST Metrics: RMSE={patchtst_metrics['RMSE']:.4f}, R2={patchtst_metrics['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Zone-wise Analysis:\")\n",
    "for zone in zones:\n",
    "    print(f\"\\n{zone}:\")\n",
    "    print(f\"  Transformer:\")\n",
    "    print(f\"    - RMSE: {results[zone]['Transformer']['RMSE']:.4f}\")\n",
    "    print(f\"    - MAE: {results[zone]['Transformer']['MAE']:.4f}\")\n",
    "    print(f\"    - MAPE: {results[zone]['Transformer']['MAPE']:.2f}%\")\n",
    "    print(f\"  PatchTST:\")\n",
    "    print(f\"    - RMSE: {results[zone]['PatchTST']['RMSE']:.4f}\")\n",
    "    print(f\"    - MAE: {results[zone]['PatchTST']['MAE']:.4f}\")\n",
    "    print(f\"    - MAPE: {results[zone]['PatchTST']['MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\n3. Key Findings:\")\n",
    "print(\"  - Model Performance:\")\n",
    "for zone in zones:\n",
    "    better_model = 'Transformer' if results[zone]['Transformer']['RMSE'] < results[zone]['PatchTST']['RMSE'] else 'PatchTST'\n",
    "    print(f\"    * {zone}: {better_model} performs better\")\n",
    "\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"  - Model Selection:\")\n",
    "print(\"    * Use Transformer model for overall better performance\")\n",
    "print(\"    * Consider ensemble approach for further improvements\")\n",
    "print(\"  - Implementation:\")\n",
    "print(\"    * Monitor performance across different time periods\")\n",
    "print(\"    * Implement regular model retraining\")\n",
    "print(\"    * Consider zone-specific model tuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
